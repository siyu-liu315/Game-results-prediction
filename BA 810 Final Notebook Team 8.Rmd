---
title: "BA 810 - Team 8 - Group Project"
author: "Tyler McMurray, Siyu Liu, Zinan Chen, Senbo Zhang, Qiuhao Chenyong"
date: "October 16, 2019"
output:
  pdf_document: default
  html_document: default
---

# League of Legends
League of Legends, known also in the gaming world as LoL for short, is a massively popular video game created by Riot Games and released in 2009. It is speculated that there are over 80 million active monthly users for League of Legends and 27 million active daily players, this game is massive by any standard. With such a large player base also comes a large fan following for its league games with professional video game players.


# We Are Looking To Predict If A Team Will Win or Lose Their Game
The problem we are looking to solve is correctly predicting a win or lose in their game. To go a bit further we want to create models that can predict if a team will win or lose their game for each minute the game plays on for until minute 60, which very few games go past this time. Each model will use the information from that minute and prior to make the best prediction. In total we should have 60 total models that will help solve our problem of predicting if the game will end in a win or lose for a certain team. 


# The Importance Of Our Predictions And The Solution We Offer
Our predictions will have signifigant use for multiple reasons. One use would be for gambling. An accruate prediction could allow for better gambling risk mitigation or arbtitrage oppurtunities. 


# The Libraries We Used For This Project

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(varhandle)
library(plyr)
library(bigrquery)
library(scales)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(glmnet)
```




### We Needed To Set A Seed For This Project For Reproducible Results

```{r}
set.seed(123456)
```


#  Creating The Final Dataset & List
We created the final dataset which is the final aggregation of all of our data into one place. We created a list which consisted of 60 parts each part had its own minute in it. Therefore, the first dataset were all minute 1 variables and games and the last was all minute 60 variables and games, and so on for each minute.

```{r}
final <- read.csv('final.csv')
final_list <- split(final, final$min)
```


# Which Model Will Offer Us The Best Predictions 
### We Are Going To Try Many Supervised Machine Learning Algorithms To Find The Most Appropriate One For Our Problem

### We Chose Minute 15 Data To Find Out Which Model Was Best
Here We created our dataset, final_15, by pulling it from the list we created when we filtered our aggregate dataset with all of the minutes so each dataset only has the correct minute in it. To be sure we utilized the unique function in order to check which minutes were in the data set. Only minute 15 was in the column, perfect. Now is also a good time to check if we properly cleaned the data again by check to see how many missing values were in our data set in total. Which there were none. 

```{r}
final_15 <- as.data.frame(final_list[15])

unique(final_15$X15.min)

sum(is.na(final_15))
```


# We Have Our Dataset and We need to Create Train and Test Datsets From Our Minute 15 Dataset, final_15
### We Are Going To Create Them For Our Linear Regression Models
We Also prepped some of the work for later by creating the variable xnames which we will use in our forward linear regression loop. 

```{r}
smp_size <- floor(0.70 * nrow(final_15))

train_ind <- sample(seq_len(nrow(final_15)), size = smp_size)

linear_train_15 <- final_15[train_ind, ]
linear_test_15 <- final_15[-train_ind, ]

xnames <- colnames(final_15)
xnames <- xnames[!xnames %in% c("X15.min", "X15.matchname", "X15.bResult")]
```


# Lets Do One Of Our Most Simplistic Models
### Linear Regression With All Parameters

```{r warning=FALSE}
linear_train_15 <- linear_train_15 %>% select(-X15.matchname, -X15.min)
linear_test_15 <- linear_test_15 %>% select(-X15.matchname, -X15.min)

linear_all_predictors <- lm(X15.bResult ~ ., data = linear_train_15)

yhat_train_15_linear <- predict(linear_all_predictors, linear_train_15)
mse_train_15_linear <- mean((linear_train_15$X15.bResult - yhat_train_15_linear) ^ 2)

yhat_test_15_linear <- predict(linear_all_predictors, linear_test_15)
mse_test_15_linear <- mean((linear_test_15$X15.bResult - yhat_test_15_linear) ^ 2)

all_predictors_train_mse <- mse_train_15_linear
all_predictors_test_mse <- mse_test_15_linear

all_predictors_train_mse
all_predictors_test_mse
```
 The ouput printed is the Train MSE and the Test MSE, respectively. This is our MSE's when all of the predictors were utilized to figure out our predictions. 
 
 
# Lets Do Some Step-Wise Regression 
### We Can Do Both Forward And Backward Selection For Our Linear Model
There are multiple ways to tackle this problem. I will show you two ways a loop for forward selection and a function for backward selection. 


#### Forward Selection
First we are going to start our formula with just an intercept and then add predictors. We are going to store the results in a tibble, which we will also utilize in our loop. 

```{r warning=FALSE}
fit_linear_15 <- lm(X15.bResult ~ 1, data = linear_train_15)

yhat_train_15_linear <- predict(fit_linear_15, linear_train_15)
mse_train_15_linear <- mean((linear_train_15$X15.bResult - yhat_train_15_linear) ^ 2)

yhat_test_15_linear <- predict(fit_linear_15, linear_test_15)
mse_test_15_linear <- mean((linear_test_15$X15.bResult - yhat_test_15_linear) ^ 2)

log_fw <-
  tibble(
    xname = "intercept",
    model = deparse(fit_linear_15$call),
    mse_train = mse_train_15_linear,
    mse_test = mse_test_15_linear
  )


while (length(xnames) > 0) {
  best_mse_train_15_linear <- NA
  best_mse_test_15_linear <- NA
  best_fit_fw_15_linear <- NA
  best_xname_15_linear <- NA
  
  
  for (xname in xnames) {
    fit_fw_tmp <- update(fit_linear_15, as.formula(paste0(". ~ . + ", xname)))
    
    
    yhat_train_tmp <- predict(fit_fw_tmp, linear_train_15)
    mse_train_tmp <- mean((linear_train_15$X15.bResult - yhat_train_tmp) ^ 2)
    
    
    yhat_test_tmp <- predict(fit_fw_tmp, linear_test_15)
    mse_test_tmp <- mean((linear_test_15$X15.bResult - yhat_test_tmp) ^ 2)
    
    
    if (is.na(best_mse_test_15_linear) | mse_test_tmp < best_mse_test_15_linear) {
      best_xname_15_linear <- xname
      best_fit_fw_15_linear <- fit_fw_tmp
      best_mse_train_15_linear <- mse_train_tmp
      best_mse_test_15_linear <- mse_test_tmp
    }
  }
  
  log_fw <-
    log_fw %>% add_row(
      xname = best_xname_15_linear,
      model = paste0(deparse(best_fit_fw_15_linear$call), collapse = ""),
      mse_train = best_mse_train_15_linear,
      mse_test = best_mse_test_15_linear
    )
  
  fit_linear_15 <- best_fit_fw_15_linear
  
  xnames <- xnames[xnames!=best_xname_15_linear]
}
```


#### Lets Take A Look At What Happened Visually 
Reading the graph from left to right shows us what happened at each iteration. Each time a new predictor was added to our formula it was added, left being the first and right being the last. We can also see how each predictor affected MSE after it was added on to the formula from the past MSE to the left.

```{r}
ggplot(log_fw, aes(seq_along(xname), mse_test)) +
  geom_point() +
  geom_line() +
  geom_point(aes(y=mse_train), color="blue") +
  geom_line(aes(y=mse_train), color="blue") +
  scale_x_continuous("Variables", labels = log_fw$xname, breaks = seq_along(log_fw$xname)) +
  scale_y_continuous("MSE test") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


#### Backward Selection
This process was much easier but does not show or store as much information as above. The end result shows the MSE for Train and Test, respectively.



```{r}
backwards_linear_15 <- linear_all_predictors

backwards_linear_15 <- step(backwards_linear_15, direction = "backward")

yhat_train_15_backwards <- predict(backwards_linear_15, linear_train_15)
mse_train_15_backwards <- mean((linear_train_15$X15.bResult - yhat_train_15_linear) ^ 2)

yhat_test_15_backwards <- predict(backwards_linear_15, linear_test_15)
mse_test_15_backwards <- mean((linear_test_15$X15.bResult - yhat_test_15_linear) ^ 2)
```


Here are the results of the computed MSE's for Train and Test, respectively. 

```{r}
mse_train_15_backwards
mse_test_15_backwards
```


```{r}
x_train <- linear_train_15 %>% select(-X15.bResult)
y_train <- linear_train_15 %>% select(X15.bResult)
x_test <- linear_test_15 %>% select(-X15.bResult)
y_test <- linear_test_15 %>% select(X15.bResult)


x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
x_test <- as.matrix(x_test)
y_test <- as.matrix(y_test)
```


# Penalized Regression For Our Problem
We know of two ways to do penalized linear regression, Ridge and Lasso. 

#### Ridge Regression 
```{r}
Ridge <- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)

Ridge_y_train_hat <- predict(Ridge, x_train, s = Ridge$lambda.min)
Ridge_y_test_hat <- predict(Ridge, x_test, s = Ridge$lambda.min)

y_train <- as.numeric(y_train)
y_test <- as.numeric(y_test)

Ridge_RSS_train <- (y_train - Ridge_y_train_hat)^2
Ridge_RSS_test <- (y_test - Ridge_y_test_hat)^2 
Ridge_mse_train <- apply(Ridge_RSS_train, 2, mean)
Ridge_mse_test <- apply(Ridge_RSS_test, 2, mean)

coef(Ridge)

Ridge_lambda_min_mse_train <- Ridge_mse_train[which.min(Ridge_mse_train)] %>% print()
Ridge_lambda_min_mse_test <- Ridge_mse_test[which.min(Ridge_mse_test)] %>% print()
```


We can see above the Coefficients of our Ridge regression. Following it is our two MSE's for this penalized Ridge Regression. They are Train MSE and Test MSE. 

#### Lasso Regression 

```{r}
Lasso <- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)

Lasso_y_train_hat <- predict(Lasso, x_train, s = Lasso$lambda.min)
Lasso_y_test_hat <- predict(Lasso, x_test, s = Lasso$lambda.min)

y_train <- as.numeric(y_train)
y_test <- as.numeric(y_test)

Lasso_RSS_train <- (y_train - Lasso_y_train_hat)^2
Lasso_RSS_test <- (y_test - Lasso_y_test_hat)^2 
Lasso_mse_train <- apply(Lasso_RSS_train, 2, mean)
Lasso_mse_test <- apply(Lasso_RSS_test, 2, mean)

coef(Lasso)

Lasso_lambda_min_mse_train <- Lasso_mse_train[which.min(Lasso_mse_train)] %>% print()
Lasso_lambda_min_mse_test <- Lasso_mse_test[which.min(Lasso_mse_test)] %>% print()
```


We can see above the Coefficients of our Lasso regression. Following it is our two MSE's for this penalized Ridge Regression. They are Train MSE and Test MSE. 


# The Truth About Linear Models For Our Problem
Our problem is inherinetly a classification one. We want to predict Win or Loss. Linear Models requires a numeric end result, we adjusted our result to, 0 for Loss and 1 for Win, but the algorithms are not being utilized correctly. These models can give us a result but its not exactly correct. We can look at these predictions, very roughly and with a grain of salt, as probability that the Blue team will win. Therefore, a .7 prediction would mean a 70% chance probability of a win. Though we did utilize it to show some interesting features realistically, we will be utilized a decision tree model. 

# Decision Tree Based Models
We can utilize a basic Tree, a Random Forest, and Boosting to get predictions. 

#### We need to put our data into a new format
Below we are preparing out data for the tree based models. We needed to convert our results into a factor to do a classification model. We are also going to prepare our formula for the trees, which includes are predictors from our base dataset. 

```{r}
train_tree <- linear_train_15
test_tree <- linear_test_15

train_tree$X15.bResult <- as.factor(train_tree$X15.bResult)
test_tree$X15.bResult <- as.factor(test_tree$X15.bResult)

f1 <- as.formula(X15.bResult ~ .)
```


#### A Basic Decision Tree

```{r}
tree_final_15 <- rpart(f1,
                  train_tree, 
                  method = "class")

yhat.train.tree <- predict(tree_final_15, train_tree)
yhat.test.tree <- predict(tree_final_15, test_tree)


colnames(yhat.train.tree) <- c("loss","win")
colnames(yhat.test.tree) <- c("loss","win")


yhat.train.tree <- as.data.frame(yhat.train.tree)
yhat.train.tree <- yhat.train.tree %>% mutate(actual = (yhat.train.tree$win > yhat.train.tree$loss))


yhat.test.tree <- as.data.frame(yhat.test.tree)
yhat.test.tree <- yhat.test.tree %>% mutate(one_is_bigger = (yhat.test.tree$win > yhat.test.tree$loss))


tree_error_rate_train <- 1 - ((length(yhat.train.tree[yhat.train.tree == TRUE])) / nrow(yhat.train.tree))
tree_error_rate_test <- 1 - ((length(yhat.test.tree[yhat.test.tree == TRUE])) / nrow(yhat.test.tree))


tree_error_rate_train
tree_error_rate_test
```


The above results show the error in their predictions for the Train and Test data respectively for our Basic Tree. You can not compare this to MSE. 

#### A Graph Of Our Tree

```{r}
rpart.plot(tree_final_15)
```


#### Random Forest 

```{r}
rf_final_15 <- randomForest(f1,
                         train_tree,
                         ntree=250,
                         do.trace=F)


rf_y_train_hat <- predict(rf_final_15, train_tree)
rf_y_test_hat <- predict(rf_final_15, test_tree)

rf_error_dataset_train <- cbind.data.frame(rf_y_train_hat, train_tree$X15.bResult)
rf_error_dataset_train <- rf_error_dataset_train %>%  
                              mutate(correct_prediction = 
                                       rf_error_dataset_train$rf_y_train_hat == rf_error_dataset_train$`train_tree$X15.bResult`)

rf_error_dataset_test <- cbind.data.frame(rf_y_test_hat, test_tree$X15.bResult)
rf_error_dataset_test <- rf_error_dataset_test %>%  
                             mutate(correct_prediction = 
                                       rf_error_dataset_test$rf_y_test_hat == rf_error_dataset_test$`test_tree$X15.bResult`)


rf_error_rate_train <- 1 - (length(rf_error_dataset_train[rf_error_dataset_train == TRUE]) / length(rf_error_dataset_train$correct_prediction))
rf_error_rate_test <- 1 - (length(rf_error_dataset_test[rf_error_dataset_test == TRUE]) / length(rf_error_dataset_test$correct_prediction))

rf_error_rate_train
rf_error_rate_test
```


Above output are the error prediction rates for the Train and Test for our Random Forest model. They can be compared to our Tree's error prediction rate but not to MSE again. We can see that our Random Forest predicts much better than our Basic Tree. 

#### A Graph Of Our Nodes

```{r}
varImpPlot(rf_final_15)
```



